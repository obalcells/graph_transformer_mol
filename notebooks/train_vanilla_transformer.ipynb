{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.copied_vanilla_transformer import make_copied_model\n",
    "from src.models.vanilla_transformer import make_model \n",
    "from src.noam_optimizer import get_noam_optimizer, NoamOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(11, 11, N=6, d_model=128, h=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training for simple sequence prediction problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_noam_optimizer(model, d_model=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"\"\"Object for holding a batch of data with mask during training.\"\"\"\n",
    "\n",
    "    def __init__(self, src, tgt=None, pad=2):  # 2 = <blank>\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if tgt is not None:\n",
    "            self.tgt = tgt[:, :-1]\n",
    "            self.tgt_y = tgt[:, 1:]\n",
    "            self.tgt_mask = self.make_std_mask(self.tgt, pad)\n",
    "            self.ntokens = (self.tgt_y != pad).data.sum()\n",
    "\n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & subsequent_mask(tgt.size(-1)).type_as(\n",
    "            tgt_mask.data\n",
    "        )\n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState:\n",
    "    \"\"\"Track number of steps, examples, and tokens processed\"\"\"\n",
    "\n",
    "    step: int = 0  # Steps in the current epoch\n",
    "    accum_step: int = 0  # Number of gradient accumulation steps\n",
    "    samples: int = 0  # total # of examples used\n",
    "    tokens: int = 0  # total # of tokens processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(reduction=\"sum\")\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, true_dist.clone().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x, crit):\n",
    "    d = x + 3 * 1\n",
    "    predict = torch.FloatTensor([[0, x / d, 1 / d, 1 / d, 1 / d]])\n",
    "    return crit(predict.log(), torch.LongTensor([1])).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"A simple loss compute and train function.\"\n",
    "\n",
    "    def __init__(self, generator, criterion):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        sloss = (\n",
    "            self.criterion(\n",
    "                x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)\n",
    "            )\n",
    "            / norm\n",
    "        )\n",
    "        return sloss.data * norm, sloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self):\n",
    "        self.param_groups = [{\"lr\": 0}]\n",
    "        None\n",
    "\n",
    "    def step(self):\n",
    "        None\n",
    "\n",
    "    def zero_grad(self, set_to_none=False):\n",
    "        None\n",
    "\n",
    "\n",
    "class DummyScheduler:\n",
    "    def step(self):\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate(step, model_size, factor, warmup):\n",
    "    \"\"\"\n",
    "    we have to default the step to 1 for LambdaLR function\n",
    "    to avoid zero raising to negative power.\n",
    "    \"\"\"\n",
    "    if step == 0:\n",
    "        step = 1\n",
    "    return factor * (\n",
    "        model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(\n",
    "    data_iter,\n",
    "    model,\n",
    "    loss_compute,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    mode=\"train\",\n",
    "    accum_iter=1,\n",
    "    train_state=TrainState(),\n",
    "):\n",
    "    \"\"\"Train a single epoch\"\"\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    n_accum = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        out = model.forward(\n",
    "            batch.src, batch.tgt, batch.src_mask, batch.tgt_mask\n",
    "        )\n",
    "        loss, loss_node = loss_compute(out, batch.tgt_y, batch.ntokens)\n",
    "        # loss_node = loss_node / accum_iter\n",
    "        if mode == \"train\" or mode == \"train+log\":\n",
    "            loss_node.backward()\n",
    "            train_state.step += 1\n",
    "            train_state.samples += batch.src.shape[0]\n",
    "            train_state.tokens += batch.ntokens\n",
    "            if i % accum_iter == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                n_accum += 1\n",
    "                train_state.accum_step += 1\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "        if i % 40 == 1 and (mode == \"train\" or mode == \"train+log\"):\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            elapsed = time.time() - start\n",
    "            print(\n",
    "                (\n",
    "                    \"Epoch Step: %6d | Accumulation Step: %3d | Loss: %6.2f \"\n",
    "                    + \"| Tokens / Sec: %7.1f | Learning Rate: %6.1e\"\n",
    "                )\n",
    "                % (i, n_accum, loss / batch.ntokens, tokens / elapsed, lr)\n",
    "            )\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "        del loss\n",
    "        del loss_node\n",
    "    return total_loss / total_tokens, train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(V, batch_size, nbatches):\n",
    "    \"Generate random data for a src-tgt copy task.\"\n",
    "    for i in range(nbatches):\n",
    "        data = torch.randint(1, V, size=(batch_size, 10))\n",
    "        data[:, 0] = 1\n",
    "        src = data.requires_grad_(False).clone().detach()\n",
    "        tgt = data.requires_grad_(False).clone().detach()\n",
    "        yield Batch(src, tgt, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.zeros(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    for i in range(max_len - 1):\n",
    "        out = model.decode(\n",
    "            memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)\n",
    "        )\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat(\n",
    "            [ys, torch.zeros(1, 1).type_as(src.data).fill_(next_word)], dim=1\n",
    "        )\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_test():\n",
    "    test_model = make_copied_model(11, 11, 2)\n",
    "    test_model.eval()\n",
    "    src = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
    "    src_mask = torch.ones(1, 1, 10)\n",
    "\n",
    "    memory = test_model.encode(src, src_mask)\n",
    "    ys = torch.zeros(1, 1).type_as(src)\n",
    "\n",
    "    for i in range(9):\n",
    "        out = test_model.decode(\n",
    "            memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)\n",
    "        )\n",
    "        prob = test_model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat(\n",
    "            [ys, torch.empty(1, 1).type_as(src.data).fill_(next_word)], dim=1\n",
    "        )\n",
    "\n",
    "    print(\"Example Untrained Model Prediction:\", ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple copy sequence task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the simple copy task.\n",
    "\n",
    "def example_simple_model(model):\n",
    "    V = 11\n",
    "    criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=0.5, betas=(0.9, 0.98), eps=1e-9\n",
    "    )\n",
    "    lr_scheduler = LambdaLR(\n",
    "        optimizer=optimizer,\n",
    "        lr_lambda=lambda step: rate(\n",
    "            step, model_size=model.src_embed[0].d_model, factor=1.0, warmup=400\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    batch_size = 80\n",
    "    for epoch in range(20):\n",
    "        model.train()\n",
    "        run_epoch(\n",
    "            data_gen(V, batch_size, 20),\n",
    "            model,\n",
    "            SimpleLossCompute(model.generator, criterion),\n",
    "            optimizer,\n",
    "            lr_scheduler,\n",
    "            mode=\"train\",\n",
    "        )\n",
    "        model.eval()\n",
    "        run_epoch(\n",
    "            data_gen(V, batch_size, 5),\n",
    "            model,\n",
    "            SimpleLossCompute(model.generator, criterion),\n",
    "            DummyOptimizer(),\n",
    "            DummyScheduler(),\n",
    "            mode=\"eval\",\n",
    "        )[0]\n",
    "\n",
    "    model.eval()\n",
    "    src = torch.LongTensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
    "    max_len = src.shape[1]\n",
    "    src_mask = torch.ones(1, 1, max_len)\n",
    "    print(\"Greedy decode:\", greedy_decode(model, src, src_mask, max_len=max_len, start_symbol=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = 11\n",
    "copied_model = make_copied_model(V, V, N=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = make_model(V, V, N=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.06 | Tokens / Sec:  2485.2 | Learning Rate: 5.5e-06\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.03 | Tokens / Sec:  2866.7 | Learning Rate: 6.1e-05\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.02 | Tokens / Sec:  2929.5 | Learning Rate: 1.2e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.02 | Tokens / Sec:  3004.8 | Learning Rate: 1.7e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.05 | Tokens / Sec:  2949.4 | Learning Rate: 2.3e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.01 | Tokens / Sec:  2856.0 | Learning Rate: 2.8e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.05 | Tokens / Sec:  2394.2 | Learning Rate: 3.4e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.03 | Tokens / Sec:  3072.0 | Learning Rate: 3.9e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.05 | Tokens / Sec:  3113.4 | Learning Rate: 4.5e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.06 | Tokens / Sec:  2792.7 | Learning Rate: 5.0e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.06 | Tokens / Sec:  3027.1 | Learning Rate: 5.6e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.02 | Tokens / Sec:  3011.9 | Learning Rate: 6.1e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.11 | Tokens / Sec:  3043.9 | Learning Rate: 6.7e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.03 | Tokens / Sec:  2992.4 | Learning Rate: 7.2e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.09 | Tokens / Sec:  2992.1 | Learning Rate: 7.8e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.08 | Tokens / Sec:  2213.9 | Learning Rate: 8.3e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.04 | Tokens / Sec:  2649.1 | Learning Rate: 8.9e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.10 | Tokens / Sec:  3026.2 | Learning Rate: 9.4e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.12 | Tokens / Sec:  3101.3 | Learning Rate: 1.0e-03\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.22 | Tokens / Sec:  3092.1 | Learning Rate: 1.1e-03\n",
      "Greedy decode: tensor([[0, 2, 1, 3, 4, 5, 6, 7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "example_simple_model(copied_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy decode: tensor([[0, 2, 1, 3, 4, 5, 6, 7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "copied_model.eval()\n",
    "src = torch.LongTensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
    "max_len = src.shape[1]\n",
    "src_mask = torch.ones(1, 1, max_len)\n",
    "print(\"Greedy decode:\", greedy_decode(copied_model, src, src_mask, max_len=max_len, start_symbol=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   3.18 | Tokens / Sec:  2376.1 | Learning Rate: 5.5e-06\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   2.14 | Tokens / Sec:  2983.6 | Learning Rate: 6.1e-05\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   1.82 | Tokens / Sec:  3050.5 | Learning Rate: 1.2e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   1.47 | Tokens / Sec:  3048.7 | Learning Rate: 1.7e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   1.15 | Tokens / Sec:  2408.8 | Learning Rate: 2.3e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.67 | Tokens / Sec:  2274.6 | Learning Rate: 2.8e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.29 | Tokens / Sec:  2865.5 | Learning Rate: 3.4e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.24 | Tokens / Sec:  3015.2 | Learning Rate: 3.9e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.22 | Tokens / Sec:  2925.7 | Learning Rate: 4.5e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.10 | Tokens / Sec:  2228.4 | Learning Rate: 5.0e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.18 | Tokens / Sec:  2047.1 | Learning Rate: 5.6e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.11 | Tokens / Sec:  2259.7 | Learning Rate: 6.1e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.15 | Tokens / Sec:  2853.7 | Learning Rate: 6.7e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.12 | Tokens / Sec:  2919.1 | Learning Rate: 7.2e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.17 | Tokens / Sec:  2846.9 | Learning Rate: 7.8e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.11 | Tokens / Sec:  2899.3 | Learning Rate: 8.3e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.08 | Tokens / Sec:  2885.7 | Learning Rate: 8.9e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.06 | Tokens / Sec:  2819.7 | Learning Rate: 9.4e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.11 | Tokens / Sec:  2929.4 | Learning Rate: 1.0e-03\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.17 | Tokens / Sec:  2925.0 | Learning Rate: 1.1e-03\n",
      "Greedy decode: tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "example_simple_model(my_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy decode: tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Greedy decode:\", greedy_decode(my_model, src, src_mask, max_len=max_len, start_symbol=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
